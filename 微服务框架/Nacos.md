##### 设计思想

```
1、本地启动三个线程池，其中一个是定时任务，此定时任务会不断轮询服务器，轮询时把本地可能发生变化的dataId和GroupId发送到nacosServer，nacosServer收到请求后会对比配置最后发生修改的时间,然后返回发生变化的dataId和GroupId，本地在根据返回的参数请求真正的数据，同时触发listener，并保存到本地缓存和文件中，
2、当文件很大时，会进行分批操作，默认没3000个key分一个任务
3、
```
#####  数据实时同步的原理
```
1、客户端发起长轮训请求，将自己感兴趣的dataId、groupId和MD5发送到服务端
2、服务端收到请求以后，先比较服务端缓存中的数据md5是否相同，如果不通，则直接返回
3、服务端会将请求hold住(长轮询),则将客户端的长轮询请求封装成一个叫ClientLongPolling的任务，交给scheduler去执行，间隔29.5S后再次检查
4、为了保证当服务端在29.5s之内发生数据变化能够及时通知给客户端，服务端采用事件订阅的方式来监听服务端本地数据变化(dumpService中触发localDataChange事件)的事件，一旦收到事件，则触发DataChangeTask的通知，并且遍历allStubs队列中的ClientLongPolling,把结果写回到客户端，就完成了一次数据的推送
如果 DataChangeTask任务完成了数据的“推送”之后，ClientLongPolling 中的调度任务又开始执行了怎么办呢？很简单，只要在进行 “推送” 操作之前，先将原来等待执行的调度任务取消掉就可以了，这样就防止了推送操作写完响应数据之后，调度任务又去写响应数据，这时肯定会报错的。所以，在ClientLongPolling方法中，最开始的一个步骤就是删除订阅事件
所以总的来说，Nacos采用推+拉的形式，客户端每隔一段长度适中的时间去服务端请求，而在这期间如果配置发生变更，服务端能够主动将变动后的结果推送给客户端，这样既能保证客户端能够实时感知到配置的变化，也降低了服务端的压力。30s是一个经验值
```

##### 选举

```
使用raft算法实现了选举，条件是过半follower要通过；
事务请求必须过leader节点，非事务请求可以在任意follower，
角色：leader、candidate(follower超时接收leader的心跳时)、follower(所有节点启动的初始状态)
过程：
1、follower节点接收到主节点的心跳超时时(150-300ms)，就会变为candidate节点，投票给自己，同时给其他所有节点发送投票请求
2、每个节点在一个term周期(同zk中epoch)，同一term只会响应一次投票请求，故如果某节点收到投票请求，且当前周期还没有响应过投票请求，就会返回投票响应，并同时重置超时时间
3、当一个节点收到过半数节点的投票时，就会变为主节点
多个节点同时投票选举时，谁的随机超时时间最短，谁就最可能成为主节点，从而避免平票的情况
```
